{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752b77db-e109-400b-8f78-d7db47fa7261",
   "metadata": {},
   "source": [
    "# Modified Catcher environment\n",
    "Made more simple and discrete.\n",
    "\n",
    "## Example\n",
    "* Red square is the fruit\n",
    "* Green square is the fruit when it was caught\n",
    "* White rectangle is the catcher (Agent)\n",
    "* White vertical lines are used as line separation\n",
    "\n",
    "Speed difference is a rendering bug, the fruits velocity is constant\n",
    "### Untrained example classical agent\n",
    "![](Saves/Run_2/Simplified_catcher_epsiode_0_steps_18.gif)\n",
    "\n",
    "### Trained example classical agent\n",
    "\n",
    "### Trained example quantum agent\n",
    "\n",
    "\n",
    "## Specifications\n",
    "per step (independent of agents action) the fruit goes lower in the screen by the same amount (jump to next higher $y$ coordinate). There is always one fruit present (sometimes it can't be seen since it is under the screen one step.)\n",
    "\n",
    "### Old State space:\n",
    "$s = [s_1,s_2,s_3]$ with:  \n",
    "* $s_1 \\in [85,255,425]=$[Left line, middle line, right lane] players $x$ centre position.\n",
    "* $s_2 \\in [85, 255, 425]=$[Left line, middle line, right lane]  fruits $x$ centre position\n",
    "* $s_3 \\in [1 , 129 , 257, 385, 481, 609]=$[Top of the screen line, intermediate position 1, intermediate position 2, intermediate position 3, bottom of the screen (catcher position), under catcher i.e. the fruit was not caught] fruits $y$ centre position\n",
    "\n",
    "### New State space:\n",
    "$s = [s_1,s_2,s_3]$ with:\n",
    "* $s_1 \\in [-1, 0, 1]=$[Left line, middle line, right lane] players $x$ centre position.\n",
    "* $s_2 \\in [-1, 0, 1]=$[Left line, middle line, right lane]  fruits $x$ centre position\n",
    "* $s_3 \\in [1.00 , 0.73 , 0.47, 0.20, 0.00, -0.27]=$[Top of the screen line, intermediate position 1, intermediate position 2, intermediate position 3, bottom of the screen (catcher position), under catcher i.e. the fruit was not caught] fruits $y$ centre position\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Action space:\n",
    " action_names = [left, stay (do nothing), right] coded as $[97,None, 100]$  \n",
    " * left: (if possible) jump one line to the left\n",
    " * stay: stay in the same lane\n",
    " * right: (if possible) jump one line to the right\n",
    "\n",
    "### rewards\n",
    "$r \\in [1.0,0.0,-1.0,-6.0]$\n",
    "* $1.0$ fruit caught\n",
    "* $0.0$ no fruit caught nor lost\n",
    "* $-1.0$ fruit fell through, $1$ live is lost of $3$ lives per default\n",
    "* $-6.0$ fruit fell through, no more lives. Game Over\n",
    "\n",
    "## Task solved criterium \n",
    "### Old idea\n",
    "The variable steps_per_episode sets the maximal reward pro episode,  as $max\\_score = \\frac{steps\\_per\\_episode}{5}$.\n",
    "If the average cumulative score (reward) of the last $n$ episodes is $\\geq max\\_score*0.95$ the task is solved (this can and should be treated as a hyperparameter). As a first test $n=500$.\n",
    "\n",
    "### Current idea\n",
    "Train for $n$ steps independent of results. With $n_q$ for Quantum and $n_c$ for classic experiments. \n",
    "\n",
    "\n",
    "## Simplifications regarding Catcher\n",
    "* Discretization of $x,y$ coordinate of fruit and catcher. $3$ lanes, positions, for $x$ and $5$ for the $y$ coordinate. This $3$ and $5$ can be changed to add complexity (in the original environment these numbers would be the pixel width and height of the game window).\n",
    "* Velocity was dropped, to have $3$ inputs and outputs, since because of discretization it becomes constant.\n",
    "* No pause between fruits, normally the fruits appear at random times, although maximal one fruit is present at once.\n",
    "* Starting live still set to $3$ as a simplification to actual situation.\n",
    "\n",
    "These simplifications can be undone  step by step to add complexity.\n",
    "\n",
    "## Results of first tests\n",
    "In these cases, $steps\\_per\\_episode=250$ then $max\\_score = 50$ for training and $steps\\_per\\_episode=1000$ then $max\\_score = 200$ for testing.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
